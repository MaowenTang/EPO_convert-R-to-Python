# -*- coding: utf-8 -*-
"""Convert R to python EPO and Cubist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iND_6WEoF2K90dtT-Texb6N_JKwz__Bf
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import libraries
import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
import matplotlib.pyplot as plt

# Step 3: Load data from your Drive
data_path = '/content/drive/My Drive/dataEPO.csv'
df = pd.read_csv(data_path)
df.columns = df.columns.str.replace('.', '_', regex=False)

# Step 4: Extract data
# Assumes columns: 'soilC', spectra0_xxx, spectra1_xxx, spectra2_xxx

soilC = df['TotalC'].values

spectra0 = df.filter(like='spectra0').values
spectra1 = df.filter(like='spectra1').values
spectra2 = df.filter(like='spectra2').values

wavelengths = [int(col.split('_')[-1]) for col in df.filter(like='spectra0_').columns]

# === Convert Absorbance to Reflectance ===
spectra0 = 10 ** (-spectra0)
spectra1 = 10 ** (-spectra1)
spectra2 = 10 ** (-spectra2)

# Step 5: Apply Savitzky-Golay smoothing
def apply_savgol(X):
    return savgol_filter(X, window_length=11, polyorder=2, deriv=0, axis=1)

spectra0_sg = apply_savgol(spectra0)
spectra1_sg = apply_savgol(spectra1)
spectra2_sg = apply_savgol(spectra2)

# Step 6: Standard Normal Variate (SNV)
def standard_normal_variate(X):
    return (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)

spectra0_snv = standard_normal_variate(spectra0_sg)
spectra1_snv = standard_normal_variate(spectra1_sg)
spectra2_snv = standard_normal_variate(spectra2_sg)

# Step 7: Define EPO function
def epo_projection_matrix(D, npc):
    dtd = D.T @ D
    _, _, Vt = np.linalg.svd(dtd)
    G = Vt[:npc].T
    Q = G @ G.T
    P = np.eye(Q.shape[0]) - Q
    return P

# Step 8: Apply EPO
npc = 3  # number of principal components to remove
D = spectra1_snv - spectra0_snv  # wet - dry
P = epo_projection_matrix(D, npc)

Z0 = spectra0_snv @ P
Z1 = spectra1_snv @ P
Z2 = spectra2_snv @ P

# Step 9: Visualize original vs corrected spectra for first sample
plt.figure(figsize=(10, 5))
plt.plot(wavelengths, spectra0_snv[0], label='Dry 5%(SNV)', color='blue')
plt.plot(wavelengths, spectra1_snv[0], label='Wet 12% (SNV)', color='green')
plt.plot(wavelengths, spectra2_snv[0], label='Semi-dry 9% (SNV)', color='red')
plt.title('Original SNV Spectra - Sample 1')
plt.xlabel('Wavelength (nm)')
plt.ylabel('Reflectance')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(wavelengths, Z0[0], label='Dry 5% (EPO)', color='blue')
plt.plot(wavelengths, Z1[0], label='Wet 12% (EPO)', color='green')
plt.plot(wavelengths, Z2[0], label='Semi-dry 9% (EPO)', color='red')
plt.title('EPO Corrected Spectra - Sample 1')
plt.xlabel('Wavelength (nm)')
plt.ylabel('Reflectance')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import pearsonr
import numpy as np

def eval_soil_metrics(y_true, y_pred, verbose=True):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # Mean Error
    ME = np.mean(y_pred - y_true)

    # Root Mean Squared Error
    RMSE = np.sqrt(mean_squared_error(y_true, y_pred))

    # Pearson correlation and r²
    rhoC, _ = pearsonr(y_true, y_pred)
    r2_corr = rhoC ** 2

    # Coefficient of determination
    R2 = r2_score(y_true, y_pred)

    # RPD: std / RMSE
    RPD = np.std(y_true, ddof=1) / RMSE

    # RPIQ: IQR / RMSE
    IQR = np.percentile(y_true, 75) - np.percentile(y_true, 25)
    RPIQ = IQR / RMSE

    if verbose:
        print("📊 Evaluation Metrics:")
        print(f"ME    = {ME:.4f}")
        print(f"RMSE  = {RMSE:.4f}")
        print(f"r²    = {r2_corr:.4f}")
        print(f"R²    = {R2:.4f}")
        print(f"ρ     = {rhoC:.4f}")
        print(f"RPD   = {RPD:.4f}")
        print(f"RPIQ  = {RPIQ:.4f}")

    return {
        'ME': ME, 'RMSE': RMSE, 'r2': r2_corr, 'R2': R2,
        'rhoC': rhoC, 'RPD': RPD, 'RPIQ': RPIQ
    }

# 例如：使用 corrected spectra Z0 拟合的模型
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(Z0, soilC)
y_pred = model.predict(Z0)

# 计算评估指标
metrics = eval_soil_metrics(soilC, y_pred)

"""# Save the EPO model for hugging face"""

# 假设你已经完成 EPO 投影矩阵计算：P = D @ V_k 之类
np.save("/content/drive/My Drive/epo_projection_matrix.npy", P)

import numpy as np

# 从 Google Drive 加载模型
P = np.load("/content/drive/My Drive/epo_projection_matrix.npy")
print("✅ EPO projection matrix loaded!")

"""# Cubist- Total Carbon Prediction"""

# Step 1: 安装 LightGBM（如未安装）
!pip install lightgbm --quiet

# Step 2: 导入必要库
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import lightgbm as lgb

# Step 3: 加载数据
df = pd.read_csv('/content/drive/My Drive/dataEPO.csv')

# Step 4: 自动查找碳含量列
target_col = [col for col in df.columns if 'C' in col or 'carbon' in col.lower()]
if not target_col:
    raise ValueError("❌ 没有找到表示总碳含量的列，请确认列名是否包含 'C' 或 'carbon'")
target_col = target_col[0]
print(f"✅ 使用的目标列为: {target_col}")

# Step 5: 特征和目标
X = df.filter(like='spectra0')  # dry reflectance 特征
y = df[target_col]              # total carbon 目标

# Step 6: 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: 训练 LightGBM 模型
model = lgb.LGBMRegressor(n_estimators=200, max_depth=6, learning_rate=0.05)
model.fit(X_train, y_train)

# Step 8: 预测与评估
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
me = np.mean(y_pred - y_test)
rho = np.corrcoef(y_test, y_pred)[0, 1]
rpd = np.std(y_test) / rmse
iqr = np.subtract(*np.percentile(y_test, [75, 25]))
rpiq = iqr / rmse

# Step 9: 输出结果
print(f"\n📊 模型评估结果：")
print(f"✅ ME     (Mean Error):           {me:.4f}")
print(f"✅ RMSE   (Root Mean Square Err): {rmse:.4f}")
print(f"✅ R²     (R-squared):             {r2:.4f}")
print(f"✅ ρ      (Pearson Correlation):  {rho:.4f}")
print(f"✅ RPD    (StdDev / RMSE):         {rpd:.4f}")
print(f"✅ RPIQ   (IQR / RMSE):            {rpiq:.4f}")

"""# Save Cubist Model"""

import joblib
import lightgbm as lgb

# 假设你已经训练好了模型：
model = lgb.LGBMRegressor(n_estimators=200, max_depth=6, learning_rate=0.05)
model.fit(X_train, y_train)

# 保存模型
joblib.dump(model, "/content/cubist_model.pkl")
print("✅ Cubist 模型已保存为 cubist_model.pkl")

# 加载模型
loaded_model = joblib.load("/content/cubist_model.pkl")

# 使用加载的模型进行预测
y_pred = loaded_model.predict(X_test)