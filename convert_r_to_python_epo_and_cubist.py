# -*- coding: utf-8 -*-
"""Convert R to python EPO and Cubist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iND_6WEoF2K90dtT-Texb6N_JKwz__Bf
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import libraries
import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
import matplotlib.pyplot as plt

# Step 3: Load data from your Drive
data_path = '/content/drive/My Drive/dataEPO.csv'
df = pd.read_csv(data_path)
df.columns = df.columns.str.replace('.', '_', regex=False)

# Step 4: Extract data
# Assumes columns: 'soilC', spectra0_xxx, spectra1_xxx, spectra2_xxx

soilC = df['TotalC'].values

spectra0 = df.filter(like='spectra0').values
spectra1 = df.filter(like='spectra1').values
spectra2 = df.filter(like='spectra2').values

wavelengths = [int(col.split('_')[-1]) for col in df.filter(like='spectra0_').columns]

# === Convert Absorbance to Reflectance ===
spectra0 = 10 ** (-spectra0)
spectra1 = 10 ** (-spectra1)
spectra2 = 10 ** (-spectra2)

# Step 5: Apply Savitzky-Golay smoothing
def apply_savgol(X):
    return savgol_filter(X, window_length=11, polyorder=2, deriv=0, axis=1)

spectra0_sg = apply_savgol(spectra0)
spectra1_sg = apply_savgol(spectra1)
spectra2_sg = apply_savgol(spectra2)

# Step 6: Standard Normal Variate (SNV)
def standard_normal_variate(X):
    return (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)

spectra0_snv = standard_normal_variate(spectra0_sg)
spectra1_snv = standard_normal_variate(spectra1_sg)
spectra2_snv = standard_normal_variate(spectra2_sg)

# Step 7: Define EPO function
def epo_projection_matrix(D, npc):
    dtd = D.T @ D
    _, _, Vt = np.linalg.svd(dtd)
    G = Vt[:npc].T
    Q = G @ G.T
    P = np.eye(Q.shape[0]) - Q
    return P

# Step 8: Apply EPO
npc = 3  # number of principal components to remove
D = spectra1_snv - spectra0_snv  # wet - dry
P = epo_projection_matrix(D, npc)

Z0 = spectra0_snv @ P
Z1 = spectra1_snv @ P
Z2 = spectra2_snv @ P

# Step 9: Visualize original vs corrected spectra for first sample
plt.figure(figsize=(10, 5))
plt.plot(wavelengths, spectra0_snv[0], label='Dry 5%(SNV)', color='blue')
plt.plot(wavelengths, spectra1_snv[0], label='Wet 12% (SNV)', color='green')
plt.plot(wavelengths, spectra2_snv[0], label='Semi-dry 9% (SNV)', color='red')
plt.title('Original SNV Spectra - Sample 1')
plt.xlabel('Wavelength (nm)')
plt.ylabel('Reflectance')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(wavelengths, Z0[0], label='Dry 5% (EPO)', color='blue')
plt.plot(wavelengths, Z1[0], label='Wet 12% (EPO)', color='green')
plt.plot(wavelengths, Z2[0], label='Semi-dry 9% (EPO)', color='red')
plt.title('EPO Corrected Spectra - Sample 1')
plt.xlabel('Wavelength (nm)')
plt.ylabel('Reflectance')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import pearsonr
import numpy as np

def eval_soil_metrics(y_true, y_pred, verbose=True):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # Mean Error
    ME = np.mean(y_pred - y_true)

    # Root Mean Squared Error
    RMSE = np.sqrt(mean_squared_error(y_true, y_pred))

    # Pearson correlation and rÂ²
    rhoC, _ = pearsonr(y_true, y_pred)
    r2_corr = rhoC ** 2

    # Coefficient of determination
    R2 = r2_score(y_true, y_pred)

    # RPD: std / RMSE
    RPD = np.std(y_true, ddof=1) / RMSE

    # RPIQ: IQR / RMSE
    IQR = np.percentile(y_true, 75) - np.percentile(y_true, 25)
    RPIQ = IQR / RMSE

    if verbose:
        print("ğŸ“Š Evaluation Metrics:")
        print(f"ME    = {ME:.4f}")
        print(f"RMSE  = {RMSE:.4f}")
        print(f"rÂ²    = {r2_corr:.4f}")
        print(f"RÂ²    = {R2:.4f}")
        print(f"Ï     = {rhoC:.4f}")
        print(f"RPD   = {RPD:.4f}")
        print(f"RPIQ  = {RPIQ:.4f}")

    return {
        'ME': ME, 'RMSE': RMSE, 'r2': r2_corr, 'R2': R2,
        'rhoC': rhoC, 'RPD': RPD, 'RPIQ': RPIQ
    }

# ä¾‹å¦‚ï¼šä½¿ç”¨ corrected spectra Z0 æ‹Ÿåˆçš„æ¨¡å‹
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(Z0, soilC)
y_pred = model.predict(Z0)

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
metrics = eval_soil_metrics(soilC, y_pred)

"""# Save the EPO model for hugging face"""

# å‡è®¾ä½ å·²ç»å®Œæˆ EPO æŠ•å½±çŸ©é˜µè®¡ç®—ï¼šP = D @ V_k ä¹‹ç±»
np.save("/content/drive/My Drive/epo_projection_matrix.npy", P)

import numpy as np

# ä» Google Drive åŠ è½½æ¨¡å‹
P = np.load("/content/drive/My Drive/epo_projection_matrix.npy")
print("âœ… EPO projection matrix loaded!")

"""# Cubist- Total Carbon Prediction"""

# Step 1: å®‰è£… LightGBMï¼ˆå¦‚æœªå®‰è£…ï¼‰
!pip install lightgbm --quiet

# Step 2: å¯¼å…¥å¿…è¦åº“
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import lightgbm as lgb

# Step 3: åŠ è½½æ•°æ®
df = pd.read_csv('/content/drive/My Drive/dataEPO.csv')

# Step 4: è‡ªåŠ¨æŸ¥æ‰¾ç¢³å«é‡åˆ—
target_col = [col for col in df.columns if 'C' in col or 'carbon' in col.lower()]
if not target_col:
    raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°è¡¨ç¤ºæ€»ç¢³å«é‡çš„åˆ—ï¼Œè¯·ç¡®è®¤åˆ—åæ˜¯å¦åŒ…å« 'C' æˆ– 'carbon'")
target_col = target_col[0]
print(f"âœ… ä½¿ç”¨çš„ç›®æ ‡åˆ—ä¸º: {target_col}")

# Step 5: ç‰¹å¾å’Œç›®æ ‡
X = df.filter(like='spectra0')  # dry reflectance ç‰¹å¾
y = df[target_col]              # total carbon ç›®æ ‡

# Step 6: åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: è®­ç»ƒ LightGBM æ¨¡å‹
model = lgb.LGBMRegressor(n_estimators=200, max_depth=6, learning_rate=0.05)
model.fit(X_train, y_train)

# Step 8: é¢„æµ‹ä¸è¯„ä¼°
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
me = np.mean(y_pred - y_test)
rho = np.corrcoef(y_test, y_pred)[0, 1]
rpd = np.std(y_test) / rmse
iqr = np.subtract(*np.percentile(y_test, [75, 25]))
rpiq = iqr / rmse

# Step 9: è¾“å‡ºç»“æœ
print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
print(f"âœ… ME     (Mean Error):           {me:.4f}")
print(f"âœ… RMSE   (Root Mean Square Err): {rmse:.4f}")
print(f"âœ… RÂ²     (R-squared):             {r2:.4f}")
print(f"âœ… Ï      (Pearson Correlation):  {rho:.4f}")
print(f"âœ… RPD    (StdDev / RMSE):         {rpd:.4f}")
print(f"âœ… RPIQ   (IQR / RMSE):            {rpiq:.4f}")

"""# Save Cubist Model"""

import joblib
import lightgbm as lgb

# å‡è®¾ä½ å·²ç»è®­ç»ƒå¥½äº†æ¨¡å‹ï¼š
model = lgb.LGBMRegressor(n_estimators=200, max_depth=6, learning_rate=0.05)
model.fit(X_train, y_train)

# ä¿å­˜æ¨¡å‹
joblib.dump(model, "/content/cubist_model.pkl")
print("âœ… Cubist æ¨¡å‹å·²ä¿å­˜ä¸º cubist_model.pkl")

# åŠ è½½æ¨¡å‹
loaded_model = joblib.load("/content/cubist_model.pkl")

# ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
y_pred = loaded_model.predict(X_test)